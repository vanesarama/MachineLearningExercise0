{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Config           Layers Activation  Learning Rate  Epochs  Accuracy  \\\n",
      "0     V1       [30, 5, 2]       tanh           0.01   10000  0.982456   \n",
      "1     V2       [30, 5, 2]       relu           0.01   10000  0.982456   \n",
      "2     V3      [30, 10, 2]       tanh           0.01   10000  0.982456   \n",
      "3     V4      [30, 10, 2]       relu           0.01   10000  0.973684   \n",
      "4     V5  [30, 10, 10, 2]       tanh           0.01   10000  0.956140   \n",
      "5     V6  [30, 10, 10, 2]       relu           0.01   10000  0.964912   \n",
      "\n",
      "   Precision    Recall  F1 Score  Learnable Parameters  Estimated RAM (MB)  \n",
      "0   0.985915  0.985915  0.985915                   167            0.001274  \n",
      "1   0.972603  1.000000  0.986111                   167            0.001274  \n",
      "2   0.985915  0.985915  0.985915                   332            0.002533  \n",
      "3   0.985714  0.971831  0.978723                   332            0.002533  \n",
      "4   0.971429  0.957746  0.964539                   442            0.003372  \n",
      "5   0.985507  0.957746  0.971429                   442            0.003372  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def relu_derivative(x): return (x > 0).astype(float)\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x): return sigmoid(x) * (1 - sigmoid(x))\n",
    "def tanh(x): return np.tanh(x)\n",
    "def tanh_derivative(x): return 1 - np.tanh(x)**2\n",
    "def softmax(x): exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)); return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "def cross_entropy(y_true, y_pred): return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]\n",
    "def cross_entropy_derivative(y_true, y_pred): return y_pred - y_true\n",
    "\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i+1]))\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "def forward_pass(X, weights, biases, activation_hidden):\n",
    "    activations, zs = [X], []\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "        zs.append(z)\n",
    "        if activation_hidden == \"relu\": a = relu(z)\n",
    "        elif activation_hidden == \"sigmoid\": a = sigmoid(z)\n",
    "        elif activation_hidden == \"tanh\": a = tanh(z)\n",
    "        activations.append(a)\n",
    "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
    "    zs.append(z)\n",
    "    activations.append(sigmoid(z))\n",
    "    return activations, zs\n",
    "\n",
    "def backward_pass(activations, zs, y_true, weights, activation_hidden):\n",
    "    grad_weights, grad_biases = [None] * len(weights), [None] * len(weights)\n",
    "    delta = cross_entropy_derivative(y_true, activations[-1])\n",
    "    grad_weights[-1] = np.dot(activations[-2].T, delta) / y_true.shape[0]\n",
    "    grad_biases[-1] = np.sum(delta, axis=0, keepdims=True) / y_true.shape[0]\n",
    "    for l in range(len(weights) - 2, -1, -1):\n",
    "        if activation_hidden == \"relu\": delta = np.dot(delta, weights[l+1].T) * relu_derivative(zs[l])\n",
    "        elif activation_hidden == \"sigmoid\": delta = np.dot(delta, weights[l+1].T) * sigmoid_derivative(zs[l])\n",
    "        elif activation_hidden == \"tanh\": delta = np.dot(delta, weights[l+1].T) * tanh_derivative(zs[l])\n",
    "        grad_weights[l] = np.dot(activations[l].T, delta) / y_true.shape[0]\n",
    "        grad_biases[l] = np.sum(delta, axis=0, keepdims=True) / y_true.shape[0]\n",
    "    return grad_weights, grad_biases\n",
    "\n",
    "def update_parameters(weights, biases, grad_weights, grad_biases, lr):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= lr * grad_weights[i]\n",
    "        biases[i] -= lr * grad_biases[i]\n",
    "    return weights, biases\n",
    "\n",
    "def predict(X, weights, biases, activation_hidden):\n",
    "    activations, _ = forward_pass(X, weights, biases, activation_hidden)\n",
    "    return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "def accuracy(y_true, y_pred): return np.mean(y_true == y_pred)\n",
    "\n",
    "def count_learnable_params(layers):\n",
    "    total = 0\n",
    "    for i in range(len(layers) - 1):\n",
    "        total += layers[i] * layers[i+1] + layers[i+1]\n",
    "    return total\n",
    "\n",
    "def estimate_memory_usage(weights, biases):\n",
    "    total_bytes = sum(w.nbytes + b.nbytes for w, b in zip(weights, biases))\n",
    "    return total_bytes / (1024 ** 2)\n",
    "\n",
    "custom_configs = [\n",
    "    {\"name\": \"V1\", \"layers\": [30, 5, 2], \"activation\": \"tanh\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V2\", \"layers\": [30, 5, 2], \"activation\": \"relu\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V3\", \"layers\": [30, 10, 2], \"activation\": \"tanh\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V4\", \"layers\": [30, 10, 2], \"activation\": \"relu\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V5\", \"layers\": [30, 10, 10, 2], \"activation\": \"tanh\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V6\", \"layers\": [30, 10, 10, 2], \"activation\": \"relu\", \"lr\": 0.01, \"epochs\": 10000}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in custom_configs:\n",
    "    weights, biases = initialize_parameters(config[\"layers\"])\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        activations_list, zs = forward_pass(X_train, weights, biases, config[\"activation\"])\n",
    "        loss = cross_entropy(y_train, activations_list[-1])\n",
    "        grad_weights, grad_biases = backward_pass(activations_list, zs, y_train, weights, config[\"activation\"])\n",
    "        weights, biases = update_parameters(weights, biases, grad_weights, grad_biases, config[\"lr\"])\n",
    "    y_pred_test = predict(X_test, weights, biases, config[\"activation\"])\n",
    "    y_true_test = np.argmax(y_test, axis=1)\n",
    "    acc = accuracy(y_true_test, y_pred_test)\n",
    "    prec = precision_score(y_true_test, y_pred_test)\n",
    "    rec = recall_score(y_true_test, y_pred_test)\n",
    "    f1 = f1_score(y_true_test, y_pred_test)\n",
    "    n_params = count_learnable_params(config[\"layers\"])\n",
    "    mem_mb = estimate_memory_usage(weights, biases)\n",
    "    results.append((\n",
    "        config[\"name\"], config[\"layers\"], config[\"activation\"], config[\"lr\"],\n",
    "        config[\"epochs\"], acc, prec, rec, f1, n_params, mem_mb\n",
    "    ))\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    \"Config\", \"Layers\", \"Activation\", \"Learning Rate\", \"Epochs\",\n",
    "    \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\",\n",
    "    \"Learnable Parameters\", \"Estimated RAM (MB)\"\n",
    "])\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
