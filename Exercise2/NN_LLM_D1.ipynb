{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Dataset and Preprocessing \n",
    "\n",
    "load_breast_cancer(): Loads the dataset (30 features, binary labels: 0/1).\n",
    "\n",
    "StandardScaler: Normalizes the input features to zero mean and unit variance.\n",
    "\n",
    "OneHotEncoder: Converts labels from shape (n, 1) to one-hot vectors like [1, 0] and [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset and Preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation and Loss Functions\n",
    "\n",
    "ReLU: max(0, x)\n",
    "\n",
    "Sigmoid: Maps to range (0, 1)\n",
    "\n",
    "Tanh: Maps to range (-1, 1)\n",
    "\n",
    "Softmax: Converts logits to probabilities over multiple classes\n",
    "\n",
    "Cross-entropy loss: For one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation and Loss Functions\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def relu_derivative(x): return (x > 0).astype(float)\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x): return sigmoid(x) * (1 - sigmoid(x))\n",
    "def tanh(x): return np.tanh(x)\n",
    "def tanh_derivative(x): return 1 - np.tanh(x)**2\n",
    "def softmax(x): exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)); return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "def cross_entropy(y_true, y_pred): return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]\n",
    "def cross_entropy_derivative(y_true, y_pred): return y_pred - y_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Initialization\n",
    "\n",
    "Createing weight matrices and bias vectors for each layer using He initialization for ReLU/tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Initialization\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i+1]))\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Pass\n",
    "\n",
    "Performing Weighted sum + activation at each layer\n",
    "\n",
    "Supports relu, sigmoid, and tanh\n",
    "\n",
    "Final layer uses sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass \n",
    "def forward_pass(X, weights, biases, activation_hidden):\n",
    "    activations, zs = [X], []\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "        zs.append(z)\n",
    "        if activation_hidden == \"relu\": a = relu(z)\n",
    "        elif activation_hidden == \"sigmoid\": a = sigmoid(z)\n",
    "        elif activation_hidden == \"tanh\": a = tanh(z)\n",
    "        activations.append(a)\n",
    "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
    "    zs.append(z)\n",
    "    activations.append(sigmoid(z))\n",
    "    return activations, zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Pass\n",
    "\n",
    "Implementing backpropagation for each layer,\n",
    "\n",
    "Use derivative of selected activation (relu, sigmoid, or tanh)\n",
    "\n",
    "Computes gradients of loss with respect to weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass\n",
    "def backward_pass(activations, zs, y_true, weights, activation_hidden):\n",
    "    grad_weights, grad_biases = [None] * len(weights), [None] * len(weights)\n",
    "    delta = cross_entropy_derivative(y_true, activations[-1])\n",
    "    grad_weights[-1] = np.dot(activations[-2].T, delta) / y_true.shape[0]\n",
    "    grad_biases[-1] = np.sum(delta, axis=0, keepdims=True) / y_true.shape[0]\n",
    "    for l in range(len(weights) - 2, -1, -1):\n",
    "        if activation_hidden == \"relu\": delta = np.dot(delta, weights[l+1].T) * relu_derivative(zs[l])\n",
    "        elif activation_hidden == \"sigmoid\": delta = np.dot(delta, weights[l+1].T) * sigmoid_derivative(zs[l])\n",
    "        elif activation_hidden == \"tanh\": delta = np.dot(delta, weights[l+1].T) * tanh_derivative(zs[l])\n",
    "        grad_weights[l] = np.dot(activations[l].T, delta) / y_true.shape[0]\n",
    "        grad_biases[l] = np.sum(delta, axis=0, keepdims=True) / y_true.shape[0]\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard SGD: param -= learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "def update_parameters(weights, biases, grad_weights, grad_biases, lr):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= lr * grad_weights[i]\n",
    "        biases[i] -= lr * grad_biases[i]\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Utilities\n",
    "\n",
    "predict(): Uses np.argmax() to get class labels from sigmoid outputs\n",
    "\n",
    "accuracy(): Direct class comparison\n",
    "\n",
    "count_learnable_params(): Weight + bias count\n",
    "\n",
    "estimate_memory_usage(): Memory used by weights + biases in MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Utilities\n",
    "def predict(X, weights, biases, activation_hidden):\n",
    "    activations, _ = forward_pass(X, weights, biases, activation_hidden)\n",
    "    return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "def accuracy(y_true, y_pred): return np.mean(y_true == y_pred)\n",
    "\n",
    "def count_learnable_params(layers):\n",
    "    total = 0\n",
    "    for i in range(len(layers) - 1):\n",
    "        total += layers[i] * layers[i+1] + layers[i+1]\n",
    "    return total\n",
    "\n",
    "def estimate_memory_usage(weights, biases):\n",
    "    total_bytes = sum(w.nbytes + b.nbytes for w, b in zip(weights, biases))\n",
    "    return total_bytes / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental Configuration and Grid Search\n",
    "\n",
    "Running 6 configurations:\n",
    "\n",
    "Different layer widths (5, 10, 10/10)\n",
    "\n",
    "Different depths (2-layer, 3-layer)\n",
    "\n",
    "Activation functions (tanh, relu)\n",
    "\n",
    "Each runs for 10,000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Configuration and Grid Search\n",
    "custom_configs = [\n",
    "    {\"name\": \"V1\", \"layers\": [30, 5, 2], \"activation\": \"tanh\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V2\", \"layers\": [30, 5, 2], \"activation\": \"relu\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V3\", \"layers\": [30, 10, 2], \"activation\": \"tanh\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V4\", \"layers\": [30, 10, 2], \"activation\": \"relu\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V5\", \"layers\": [30, 10, 10, 2], \"activation\": \"tanh\", \"lr\": 0.01, \"epochs\": 10000},\n",
    "    {\"name\": \"V6\", \"layers\": [30, 10, 10, 2], \"activation\": \"relu\", \"lr\": 0.01, \"epochs\": 10000}\n",
    "]\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Training Loop\n",
    "\n",
    "For each configuration:\n",
    "\n",
    "Initialize weights/biases\n",
    "\n",
    "Train using forward/backward passes\n",
    "\n",
    "Predict test set\n",
    "\n",
    "Evaluate: accuracy, precision, recall, F1\n",
    "\n",
    "Log results to a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Loop\n",
    "for config in custom_configs:\n",
    "    weights, biases = initialize_parameters(config[\"layers\"])\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        activations_list, zs = forward_pass(X_train, weights, biases, config[\"activation\"])\n",
    "        loss = cross_entropy(y_train, activations_list[-1])\n",
    "        grad_weights, grad_biases = backward_pass(activations_list, zs, y_train, weights, config[\"activation\"])\n",
    "        weights, biases = update_parameters(weights, biases, grad_weights, grad_biases, config[\"lr\"])\n",
    "    y_pred_test = predict(X_test, weights, biases, config[\"activation\"])\n",
    "    y_true_test = np.argmax(y_test, axis=1)\n",
    "    acc = accuracy(y_true_test, y_pred_test)\n",
    "    prec = precision_score(y_true_test, y_pred_test)\n",
    "    rec = recall_score(y_true_test, y_pred_test)\n",
    "    f1 = f1_score(y_true_test, y_pred_test)\n",
    "    n_params = count_learnable_params(config[\"layers\"])\n",
    "    mem_mb = estimate_memory_usage(weights, biases)\n",
    "    results.append((\n",
    "        config[\"name\"], config[\"layers\"], config[\"activation\"], config[\"lr\"],\n",
    "        config[\"epochs\"], acc, prec, rec, f1, n_params, mem_mb\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Config           Layers Activation  Learning Rate  Epochs  Accuracy  \\\n",
      "0     V1       [30, 5, 2]       tanh           0.01   10000  0.982456   \n",
      "1     V2       [30, 5, 2]       relu           0.01   10000  0.982456   \n",
      "2     V3      [30, 10, 2]       tanh           0.01   10000  0.982456   \n",
      "3     V4      [30, 10, 2]       relu           0.01   10000  0.973684   \n",
      "4     V5  [30, 10, 10, 2]       tanh           0.01   10000  0.973684   \n",
      "5     V6  [30, 10, 10, 2]       relu           0.01   10000  0.973684   \n",
      "\n",
      "   Precision    Recall  F1 Score  Learnable Parameters  Estimated RAM (MB)  \n",
      "0   0.985915  0.985915  0.985915                   167            0.001274  \n",
      "1   0.985915  0.985915  0.985915                   167            0.001274  \n",
      "2   0.985915  0.985915  0.985915                   332            0.002533  \n",
      "3   0.972222  0.985915  0.979021                   332            0.002533  \n",
      "4   0.985714  0.971831  0.978723                   442            0.003372  \n",
      "5   0.985714  0.971831  0.978723                   442            0.003372  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Output Results\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    \"Config\", \"Layers\", \"Activation\", \"Learning Rate\", \"Epochs\",\n",
    "    \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\",\n",
    "    \"Learnable Parameters\", \"Estimated RAM (MB)\"\n",
    "])\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
