{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import Required Libraries\n",
    "\n",
    "import torch                          # PyTorch library for building and training neural networks\n",
    "import torch.nn as nn                 # Neural network layers and components (e.g., Linear, ReLU)\n",
    "import numpy as np                   # For numerical operations (arrays, random numbers, etc.)\n",
    "from sklearn.datasets import fetch_covtype          # Load the Covertype dataset (forest cover types)\n",
    "from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler      # To normalize feature values\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # Evaluation metrics\n",
    "import pandas as pd                  # For creating dataframes and saving results as CSV\n",
    "import matplotlib.pyplot as plt      # For plotting and visualization\n",
    "import os                            # Used with psutil to track system resources\n",
    "import psutil                        # To measure memory usage of the current process\n",
    "import time                          # To track training time\n",
    "\n",
    "#  Function to Print Current RAM Usage\n",
    "def print_memory_usage(label=\"\"):\n",
    "    process = psutil.Process(os.getpid())             # Get current Python process info\n",
    "    mem_mb = process.memory_info().rss / 1024 / 1024  # Convert memory usage to MB\n",
    "    print(f\"[{label}] RAM usage: {mem_mb:.2f} MB\")     # Print memory with a custom label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility (same behavior each time the code is run)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#  Choose whether to use GPU (if available) or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#  Print the device being used\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load and Preprocess Covertype Dataset\n",
    "\n",
    "# Load the Covertype dataset (predicts forest cover types based on cartographic variables)\n",
    "data = fetch_covtype()\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and test sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features so they have mean 0 and standard deviation 1\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)   # Fit scaler on training data and transform it\n",
    "X_test = scaler.transform(X_test)         # Transform test data using the same scaler\n",
    "\n",
    "# Convert data to PyTorch tensors and move them to the selected device (CPU or GPU)\n",
    "X_train = torch.FloatTensor(X_train).to(device)\n",
    "X_test = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "# Convert labels to LongTensors and shift class labels from 1–7 to 0–6 (PyTorch expects 0-based labels)\n",
    "y_train = torch.LongTensor(y_train - 1).to(device)\n",
    "y_test = torch.LongTensor(y_test - 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Neural Network Class Definition\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, hidden_activation='tanh', dropout_rate=0.2):\n",
    "        super(NeuralNetwork, self).__init__()  # Call the constructor of nn.Module\n",
    "        \n",
    "        layers = []               # List to hold all the layers of the network\n",
    "        prev_size = input_size    # Start with input size\n",
    "\n",
    "        # Add hidden layers dynamically based on the 'hidden_sizes' list\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))  # Fully connected layer\n",
    "            layers.append(nn.Tanh() if hidden_activation == 'tanh' else nn.ReLU())  # Activation function\n",
    "            layers.append(nn.Dropout(dropout_rate))            # Dropout to reduce overfitting\n",
    "            prev_size = hidden_size                            # Update input size for next layer\n",
    "\n",
    "        #  Add the final output layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "\n",
    "        #  Wrap all layers in a sequential container for easy forward pass\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        #  Initialize weights of all linear layers\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                if hidden_activation == 'relu':\n",
    "                    nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')  # Better for ReLU\n",
    "                else:\n",
    "                    nn.init.xavier_normal_(layer.weight)                        # Better for Tanh\n",
    "                nn.init.zeros_(layer.bias)                                      # Initialize bias to 0\n",
    "\n",
    "    #  Define the forward pass\n",
    "    def forward(self, x):\n",
    "        return self.network(x)  # Simply pass the input through all the layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Training Function with Evaluation and Early Stopping\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs, batch_size, lr, patience=10):\n",
    "    #  Loss function for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #  Use Adam optimizer for faster convergence\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Calculate number of batches per epoch\n",
    "    n_batches = len(X_train) // batch_size\n",
    "\n",
    "    #  Dictionary to store accuracy and loss during training\n",
    "    results = {'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': []}\n",
    "\n",
    "    #  Initialize early stopping variables\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    #  Evaluate model on a fixed subset of 10,000 samples (faster evaluation)\n",
    "    eval_train_size = 10000\n",
    "    eval_test_size = 10000\n",
    "    train_subset_idx = torch.randperm(len(X_train))[:eval_train_size].to(device)\n",
    "    test_subset_idx = torch.randperm(len(X_test))[:eval_test_size].to(device)\n",
    "\n",
    "    #  Start training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()            # Set model to training mode\n",
    "        epoch_loss = 0           # Track total loss for this epoch\n",
    "\n",
    "        # Shuffle the training indices\n",
    "        indices = torch.randperm(len(X_train)).to(device)\n",
    "\n",
    "        # Train on mini-batches\n",
    "        for i in range(n_batches):\n",
    "            batch_idx = indices[i * batch_size:(i + 1) * batch_size]\n",
    "            X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "\n",
    "            optimizer.zero_grad()               # Reset gradients\n",
    "            outputs = model(X_batch)            # Forward pass\n",
    "            loss = criterion(outputs, y_batch)  # Compute loss\n",
    "            loss.backward()                     # Backpropagation\n",
    "            optimizer.step()                    # Update weights\n",
    "            epoch_loss += loss.item()           # Accumulate loss for reporting\n",
    "\n",
    "        #  Evaluation on train and test subsets (no gradients needed)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Training subset\n",
    "            train_outputs = model(X_train[train_subset_idx])\n",
    "            train_loss = criterion(train_outputs, y_train[train_subset_idx]).item()\n",
    "            train_pred = torch.argmax(train_outputs, dim=1)\n",
    "            train_acc = accuracy_score(y_train[train_subset_idx].cpu().numpy(), train_pred.cpu().numpy())\n",
    "\n",
    "            # Test subset\n",
    "            test_outputs = model(X_test[test_subset_idx])\n",
    "            test_loss = criterion(test_outputs, y_test[test_subset_idx]).item()\n",
    "            test_pred = torch.argmax(test_outputs, dim=1)\n",
    "            test_acc = accuracy_score(y_test[test_subset_idx].cpu().numpy(), test_pred.cpu().numpy())\n",
    "\n",
    "        # Save metrics\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['test_loss'].append(test_loss)\n",
    "\n",
    "        #  Print progress every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Avg Loss: {epoch_loss / n_batches:.4f}\")\n",
    "\n",
    "        # Early stopping based on test loss\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    #  Return training metrics for visualization or analysis\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions and Experiment Configurations\n",
    "\n",
    "# Function to count the total number of trainable parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#  Estimate how much memory the model uses (in bytes), assuming 4 bytes per parameter (float32)\n",
    "def estimate_ram(parameters):\n",
    "    return parameters * 4\n",
    "\n",
    "\n",
    "#  List of configurations for experimenting with different network structures and hyperparameters\n",
    "configs = [\n",
    "    # 2 hidden layers with 64 neurons each, using Tanh activation\n",
    "    {'hidden_sizes': [64, 64], 'activation': 'tanh', 'lr': 0.001, 'epochs': 500},\n",
    "\n",
    "    # 2 hidden layers with 64 neurons each, using ReLU activation and smaller learning rate\n",
    "    {'hidden_sizes': [64, 64], 'activation': 'relu', 'lr': 0.0001, 'epochs': 500},\n",
    "\n",
    "    # 4 hidden layers with 128 neurons each, using Tanh\n",
    "    {'hidden_sizes': [128, 128, 128, 128], 'activation': 'tanh', 'lr': 0.001, 'epochs': 500},\n",
    "\n",
    "    # 4 hidden layers with 128 neurons each, using ReLU and a smaller learning rate\n",
    "    {'hidden_sizes': [128, 128, 128, 128], 'activation': 'relu', 'lr': 0.0001, 'epochs': 500},\n",
    "\n",
    "    # Single hidden layer with 64 neurons and ReLU activation\n",
    "    {'hidden_sizes': [64], 'activation': 'relu', 'lr': 0.0001, 'epochs': 500}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 1: [64, 64] neurons, tanh activation\n",
      "[Before Training Config 1] RAM usage: 691.41 MB\n",
      "Epoch: 0, Avg Loss: 0.7457\n",
      "Epoch: 100, Avg Loss: 0.4642\n",
      "Early stopping at epoch 169\n",
      "Config 1 took 9.77 minutes.\n",
      "[After Training Config 1] RAM usage: 482.88 MB\n",
      "Training Accuracy: 0.8544\n",
      "Test Accuracy: 0.8524\n",
      "Total Parameters: 8135, Estimated RAM: 32540 bytes\n",
      "\n",
      "Testing Configuration 2: [64, 64] neurons, relu activation\n",
      "[Before Training Config 2] RAM usage: 731.28 MB\n",
      "Epoch: 0, Avg Loss: 1.3224\n",
      "Epoch: 100, Avg Loss: 0.4911\n",
      "Epoch: 200, Avg Loss: 0.4612\n",
      "Epoch: 300, Avg Loss: 0.4477\n",
      "Early stopping at epoch 302\n",
      "Config 2 took 23.75 minutes.\n",
      "[After Training Config 2] RAM usage: 162.56 MB\n",
      "Training Accuracy: 0.8477\n",
      "Test Accuracy: 0.8450\n",
      "Total Parameters: 8135, Estimated RAM: 32540 bytes\n",
      "\n",
      "Testing Configuration 3: [128, 128, 128, 128] neurons, tanh activation\n",
      "[Before Training Config 3] RAM usage: 474.44 MB\n",
      "Epoch: 0, Avg Loss: 0.6902\n",
      "Epoch: 100, Avg Loss: 0.3550\n",
      "Early stopping at epoch 144\n",
      "Config 3 took 20.53 minutes.\n",
      "[After Training Config 3] RAM usage: 184.03 MB\n",
      "Training Accuracy: 0.9078\n",
      "Test Accuracy: 0.9034\n",
      "Total Parameters: 57479, Estimated RAM: 229916 bytes\n",
      "\n",
      "Testing Configuration 4: [128, 128, 128, 128] neurons, relu activation\n",
      "[Before Training Config 4] RAM usage: 251.58 MB\n",
      "Epoch: 0, Avg Loss: 1.1171\n",
      "Epoch: 100, Avg Loss: 0.3977\n",
      "Epoch: 200, Avg Loss: 0.3506\n",
      "Epoch: 300, Avg Loss: 0.3319\n",
      "Early stopping at epoch 396\n",
      "Config 4 took 76.33 minutes.\n",
      "[After Training Config 4] RAM usage: 157.15 MB\n",
      "Training Accuracy: 0.9160\n",
      "Test Accuracy: 0.9109\n",
      "Total Parameters: 57479, Estimated RAM: 229916 bytes\n",
      "\n",
      "Testing Configuration 5: [64] neurons, relu activation\n",
      "[Before Training Config 5] RAM usage: 192.88 MB\n",
      "Epoch: 0, Avg Loss: 1.4087\n",
      "Epoch: 100, Avg Loss: 0.5279\n",
      "Epoch: 200, Avg Loss: 0.5113\n",
      "Epoch: 300, Avg Loss: 0.5046\n",
      "Epoch: 400, Avg Loss: 0.5001\n",
      "Config 5 took 23.86 minutes.\n",
      "[After Training Config 5] RAM usage: 162.21 MB\n",
      "Training Accuracy: 0.8010\n",
      "Test Accuracy: 0.7994\n",
      "Total Parameters: 3975, Estimated RAM: 15900 bytes\n"
     ]
    }
   ],
   "source": [
    "#  Run Experiments for Different Neural Network Configurations\n",
    "\n",
    "# Get the number of input features from the training data\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "#  Get the number of output classes (should be 7 for Covertype dataset)\n",
    "output_size = len(np.unique(y))\n",
    "\n",
    "# Define the batch size for training\n",
    "batch_size = 512\n",
    "\n",
    "#  List to store results of each configuration\n",
    "results_summary = []\n",
    "\n",
    "#  Loop through all configurations defined earlier\n",
    "for idx, config in enumerate(configs, 1):\n",
    "    print(f\"\\nTesting Configuration {idx}: {config['hidden_sizes']} neurons, {config['activation']} activation\")\n",
    "\n",
    "    #  Create a neural network model using current configuration\n",
    "    model = NeuralNetwork(input_size, config['hidden_sizes'], output_size, config['activation']).to(device)\n",
    "\n",
    "    #  Print memory usage before training\n",
    "    print_memory_usage(f\"Before Training Config {idx}\")\n",
    "\n",
    "    #  Start timer to measure how long training takes\n",
    "    start_time = time.time()\n",
    "\n",
    "    #  Train the model with current config's parameters\n",
    "    results = train_model(\n",
    "        model,\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        config['epochs'],\n",
    "        batch_size,\n",
    "        config['lr']\n",
    "    )\n",
    "\n",
    "    #  Stop timer and calculate elapsed time in minutes\n",
    "    end_time = time.time()\n",
    "    elapsed = (end_time - start_time) / 60\n",
    "    print(f\"Config {idx} took {elapsed:.2f} minutes.\")\n",
    "\n",
    "    #  Print memory usage after training\n",
    "    print_memory_usage(f\"After Training Config {idx}\")\n",
    "\n",
    "    #  Evaluate model on full training and test sets\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train)\n",
    "        train_pred = torch.argmax(train_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        test_outputs = model(X_test)\n",
    "        test_pred = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    #  Compute training and test metrics\n",
    "    train_acc = accuracy_score(y_train.cpu().numpy(), train_pred)\n",
    "    test_acc = accuracy_score(y_test.cpu().numpy(), test_pred)\n",
    "\n",
    "    train_precision = precision_score(y_train.cpu().numpy(), train_pred, average='weighted')\n",
    "    test_precision = precision_score(y_test.cpu().numpy(), test_pred, average='weighted')\n",
    "\n",
    "    train_recall = recall_score(y_train.cpu().numpy(), train_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test.cpu().numpy(), test_pred, average='weighted')\n",
    "\n",
    "    train_f1 = f1_score(y_train.cpu().numpy(), train_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test.cpu().numpy(), test_pred, average='weighted')\n",
    "\n",
    "    # Count total trainable parameters\n",
    "    params = count_parameters(model)\n",
    "\n",
    "    #  Estimate memory usage based on parameter count\n",
    "    ram = estimate_ram(params)\n",
    "\n",
    "    #  Print accuracy and resource usage for this configuration\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Total Parameters: {params}, Estimated RAM: {ram} bytes\")\n",
    "\n",
    "    #  Store all results for this configuration\n",
    "    results_summary.append({\n",
    "        'Config': f'Config {idx}',\n",
    "        'Hidden Sizes': config['hidden_sizes'],\n",
    "        'Activation': config['activation'],\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Train Precision': train_precision,\n",
    "        'Test Precision': test_precision,\n",
    "        'Train Recall': train_recall,\n",
    "        'Test Recall': test_recall,\n",
    "        'Train F1': train_f1,\n",
    "        'Test F1': test_f1,\n",
    "        'Parameters': params,\n",
    "        'RAM (bytes)': ram,\n",
    "        'Time (min)': elapsed\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Plot Results\n",
    "\n",
    "# Convert the results list into a pandas DataFrame\n",
    "df_results = pd.DataFrame(results_summary)\n",
    "\n",
    "#  Save the DataFrame to a CSV file for later use or reporting\n",
    "df_results.to_csv('pytorch_covtype_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'pytorch_covtype_results.csv'\")\n",
    "\n",
    "# Plot Train vs Test Accuracy for each configuration\n",
    "\n",
    "# Set the figure size (width, height)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create x-axis positions for each configuration\n",
    "x = np.arange(len(configs))\n",
    "\n",
    "# Draw bar chart for training accuracy\n",
    "plt.bar(x - 0.2, [r['Train Accuracy'] for r in results_summary], 0.4, label='Train Accuracy')\n",
    "\n",
    "# Draw bar chart for test accuracy\n",
    "plt.bar(x + 0.2, [r['Test Accuracy'] for r in results_summary], 0.4, label='Test Accuracy')\n",
    "\n",
    "# Label the x-axis and y-axis\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Set chart title\n",
    "plt.title('PyTorch Train vs Test Accuracy on Covertype Dataset')\n",
    "\n",
    "# Format x-tick labels to show configuration info\n",
    "plt.xticks(\n",
    "    x,\n",
    "    [f'Config {i+1}\\n{r[\"Hidden Sizes\"]}\\n{r[\"Activation\"]}' for i, r in enumerate(results_summary)],\n",
    "    rotation=45\n",
    ")\n",
    "\n",
    "# Add legend to distinguish train and test bars\n",
    "plt.legend()\n",
    "\n",
    "# Automatically adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig('pytorch_covtype_accuracy_plot.png')\n",
    "\n",
    "#  Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
