{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Config                       Layers Activation  Learning Rate  Epochs  \\\n",
      "0     V1              [54, 64, 64, 7]       tanh          0.005     500   \n",
      "1     V2              [54, 64, 64, 7]       relu          0.005     500   \n",
      "2     V3  [54, 128, 128, 128, 128, 7]       tanh          0.005     500   \n",
      "3     V4  [54, 128, 128, 128, 128, 7]       relu          0.001     500   \n",
      "\n",
      "   Batch Size  Accuracy  Precision    Recall  F1 Score  Learnable Parameters  \\\n",
      "0         128  0.919692   0.919263  0.919692  0.919328                  8135   \n",
      "1         128  0.894762   0.896862  0.894762  0.893100                  8135   \n",
      "2         128  0.963486   0.963478  0.963486  0.963473                 57479   \n",
      "3         128  0.926749   0.926656  0.926749  0.926018                 57479   \n",
      "\n",
      "   Estimated RAM (MB)  \n",
      "0            0.062065  \n",
      "1            0.062065  \n",
      "2            0.438530  \n",
      "3            0.438530  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "data = fetch_covtype()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def relu_derivative(x): return (x > 0).astype(float)\n",
    "def tanh(x): return np.tanh(x)\n",
    "def tanh_derivative(x): return 1 - np.tanh(x)**2\n",
    "def softmax(x): exp = np.exp(x - np.max(x, axis=1, keepdims=True)); return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "def cross_entropy(y_true, y_pred): return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]\n",
    "def cross_entropy_derivative(y_true, y_pred): return y_pred - y_true\n",
    "\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2. / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i + 1]))\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "def forward_pass(X, weights, biases, activation_hidden):\n",
    "    activations, zs = [X], []\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "        zs.append(z)\n",
    "        if activation_hidden == \"relu\": a = relu(z)\n",
    "        elif activation_hidden == \"tanh\": a = tanh(z)\n",
    "        activations.append(a)\n",
    "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
    "    zs.append(z)\n",
    "    activations.append(softmax(z))\n",
    "    return activations, zs\n",
    "\n",
    "def backward_pass(activations, zs, y_true, weights, activation_hidden):\n",
    "    grad_weights, grad_biases = [None] * len(weights), [None] * len(weights)\n",
    "    delta = cross_entropy_derivative(y_true, activations[-1])\n",
    "    grad_weights[-1] = np.dot(activations[-2].T, delta) / y_true.shape[0]\n",
    "    grad_biases[-1] = np.sum(delta, axis=0, keepdims=True) / y_true.shape[0]\n",
    "    for l in range(len(weights) - 2, -1, -1):\n",
    "        if activation_hidden == \"relu\": delta = np.dot(delta, weights[l + 1].T) * relu_derivative(zs[l])\n",
    "        elif activation_hidden == \"tanh\": delta = np.dot(delta, weights[l + 1].T) * tanh_derivative(zs[l])\n",
    "        grad_weights[l] = np.dot(activations[l].T, delta) / y_true.shape[0]\n",
    "        grad_biases[l] = np.sum(delta, axis=0, keepdims=True) / y_true.shape[0]\n",
    "    return grad_weights, grad_biases\n",
    "\n",
    "def update_parameters(weights, biases, grad_weights, grad_biases, lr):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= lr * grad_weights[i]\n",
    "        biases[i] -= lr * grad_biases[i]\n",
    "    return weights, biases\n",
    "\n",
    "def predict(X, weights, biases, activation_hidden):\n",
    "    activations, _ = forward_pass(X, weights, biases, activation_hidden)\n",
    "    return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "def accuracy(y_true, y_pred): return np.mean(y_true == y_pred)\n",
    "def count_learnable_params(layers): return sum(layers[i] * layers[i+1] + layers[i+1] for i in range(len(layers)-1))\n",
    "def estimate_memory_usage(weights, biases): return sum(w.nbytes + b.nbytes for w, b in zip(weights, biases)) / (1024 ** 2)\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"V1\", \"layers\": [X_train.shape[1], 64, 64, y_train.shape[1]], \"activation\": \"tanh\", \"lr\": 0.005, \"epochs\": 500, \"batch_size\": 128},\n",
    "    {\"name\": \"V2\", \"layers\": [X_train.shape[1], 64, 64, y_train.shape[1]], \"activation\": \"relu\", \"lr\": 0.005, \"epochs\": 500, \"batch_size\": 128},\n",
    "    {\"name\": \"V3\", \"layers\": [X_train.shape[1], 128, 128, 128, 128, y_train.shape[1]], \"activation\": \"tanh\", \"lr\": 0.005, \"epochs\": 500, \"batch_size\": 128},\n",
    "    {\"name\": \"V4\", \"layers\": [X_train.shape[1], 128, 128, 128, 128, y_train.shape[1]], \"activation\": \"relu\", \"lr\": 0.001, \"epochs\": 500, \"batch_size\": 128}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    weights, biases = initialize_parameters(config[\"layers\"])\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        for i in range(0, X_train.shape[0], config[\"batch_size\"]):\n",
    "            X_batch = X_train[i:i+config[\"batch_size\"]]\n",
    "            y_batch = y_train[i:i+config[\"batch_size\"]]\n",
    "            activations_list, zs = forward_pass(X_batch, weights, biases, config[\"activation\"])\n",
    "            grad_weights, grad_biases = backward_pass(activations_list, zs, y_batch, weights, config[\"activation\"])\n",
    "            weights, biases = update_parameters(weights, biases, grad_weights, grad_biases, config[\"lr\"])\n",
    "    y_pred_test = predict(X_test, weights, biases, config[\"activation\"])\n",
    "    y_true_test = np.argmax(y_test, axis=1)\n",
    "    acc = accuracy(y_true_test, y_pred_test)\n",
    "    prec = precision_score(y_true_test, y_pred_test, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(y_true_test, y_pred_test, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(y_true_test, y_pred_test, average=\"weighted\", zero_division=0)\n",
    "    n_params = count_learnable_params(config[\"layers\"])\n",
    "    mem_mb = estimate_memory_usage(weights, biases)\n",
    "    results.append((\n",
    "        config[\"name\"], config[\"layers\"], config[\"activation\"], config[\"lr\"], config[\"epochs\"],\n",
    "        config[\"batch_size\"], acc, prec, rec, f1, n_params, mem_mb\n",
    "    ))\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    \"Config\", \"Layers\", \"Activation\", \"Learning Rate\", \"Epochs\", \"Batch Size\",\n",
    "    \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Learnable Parameters\", \"Estimated RAM (MB)\"\n",
    "])\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
