{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Avg Loss: 0.5417\n",
      "Epoch: 100 \t Avg Loss: 0.3094\n",
      "Epoch: 200 \t Avg Loss: 1.2059\n",
      "Epoch: 300 \t Avg Loss: 1.2059\n",
      "Epoch: 400 \t Avg Loss: 1.2060\n",
      "TRAINING RESULTS: \n",
      "\n",
      "Accuracy:  0.4879\n",
      "Precision: 0.2381\n",
      "Recall:    0.4879\n",
      "F1 score:  0.3200\n",
      "\n",
      "TEST RESULTS: \n",
      "\n",
      "Loss:      1.2061\n",
      "Accuracy:  0.4862\n",
      "Precision: 0.2364\n",
      "Recall:    0.4862\n",
      "F1 score:  0.3181\n",
      "\n",
      "Total Learnable Parameters: 8135\n",
      "Peak RAM usage: 1733.68 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Three Layers, 64 neurons, ReLU in hidden layers, softmax in output layers\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tracemalloc\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "data = fetch_covtype()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "layers = [X_train.shape[1], 64, 64, y_train.shape[1]]\n",
    "\n",
    "# WEIGHTS AND BIAS ------------------------------\n",
    "\n",
    "def weight_and_bias(layers):\n",
    "    weight = []\n",
    "    bias = []\n",
    "    np.random.seed(0)\n",
    "    for i in range(len(layers)-1):\n",
    "        W = np.random.randn(layers[i], layers[i+1]) * np.sqrt(1. / layers[i])\n",
    "        weight.append(W)\n",
    "        b = np.zeros((1, layers[i+1]))\n",
    "        bias.append(b)\n",
    "    return weight, bias\n",
    "\n",
    "weight, bias = weight_and_bias(layers)\n",
    "\n",
    "# ACTIVATION FUNCTIONS --------------------------\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(x):\n",
    "    x = np.clip(x, -100, 100)\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    t = tanh(x)\n",
    "    return 1 - t**2\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(z):\n",
    "    z_exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def softmax_cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "# FORWARD PROPAGATION ----------------------------\n",
    "\n",
    "def forward_propagation(X, weight, bias):\n",
    "    Z = []\n",
    "    A = [X]\n",
    "    for i in range(len(layers) - 1):\n",
    "        Z_cur = np.dot(A[i], weight[i]) + bias[i]\n",
    "        if i < len(layers) - 2:\n",
    "            A_cur = ReLU(Z_cur)\n",
    "        else:\n",
    "            A_cur = softmax(Z_cur)\n",
    "        Z.append(Z_cur)\n",
    "        A.append(A_cur)\n",
    "    return Z, A\n",
    "\n",
    "# BACKWARD PROPAGATION ---------------------------\n",
    "\n",
    "def backward_propagation(layers, y_true, A, Z, weight, bias, learning_rate):\n",
    "    n_layer = len(layers) - 1\n",
    "    dA = [0] * (n_layer - 1)\n",
    "    dZ = [0] * n_layer\n",
    "    dW = [0] * n_layer\n",
    "    db = [0] * n_layer\n",
    "\n",
    "    dZ[n_layer - 1] = softmax_cross_entropy_derivative(y_true, A[n_layer])\n",
    "    dW[n_layer - 1] = np.dot(A[n_layer - 1].T, dZ[n_layer - 1])\n",
    "    db[n_layer - 1] = np.sum(dZ[n_layer - 1], axis=0, keepdims=True)\n",
    "\n",
    "    for i in reversed(range(len(layers) - 2)):\n",
    "        dA[i] = np.dot(dZ[i + 1], weight[i + 1].T)\n",
    "        dZ[i] = dA[i] * ReLU_derivative(Z[i])\n",
    "        dW[i] = np.dot(A[i].T, dZ[i])\n",
    "        db[i] = np.sum(dZ[i], axis=0, keepdims=True)\n",
    "\n",
    "    for i in reversed(range(len(layers) - 1)):\n",
    "        weight[i] -= learning_rate * dW[i]\n",
    "        bias[i] -= learning_rate * db[i]\n",
    "\n",
    "    return weight, bias\n",
    "\n",
    "# TRAINING  --------------------------\n",
    "\n",
    "learning_rate = 0.005\n",
    "n_epoch = 500\n",
    "batch_size = 128\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_batches = n_samples // batch_size\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "\n",
    "        Z, A = forward_propagation(X_batch, weight, bias)\n",
    "        loss = categorical_cross_entropy(y_batch, A[-1])\n",
    "        weight, bias = backward_propagation(layers, y_batch, A, Z, weight, bias, learning_rate)\n",
    "        epoch_loss += loss\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} \\t Avg Loss: {epoch_loss / n_batches:.4f}\")\n",
    "\n",
    "Z_train, A_train = forward_propagation(X_train, weight, bias)\n",
    "y_pred_train = np.argmax(A_train[-1], axis=1)\n",
    "y_true_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "print(\"TRAINING RESULTS: \\n\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_true_train, y_pred_train):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_true_train, y_pred_train, average='weighted'):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_true_train, y_pred_train, average='weighted'):.4f}\")\n",
    "print(f\"F1 score:  {f1_score(y_true_train, y_pred_train, average='weighted'):.4f}\")\n",
    "\n",
    "# TEST  --------------------------------\n",
    "\n",
    "Z_test, A_test = forward_propagation(X_test, weight, bias)\n",
    "loss = categorical_cross_entropy(y_test, A_test[-1])\n",
    "y_pred_test = np.argmax(A_test[-1], axis=1)\n",
    "y_true_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nTEST RESULTS: \\n\")\n",
    "print(f\"Loss:      {loss:.4f}\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_true_test, y_pred_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_true_test, y_pred_test, average='weighted'):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_true_test, y_pred_test, average='weighted'):.4f}\")\n",
    "print(f\"F1 score:  {f1_score(y_true_test, y_pred_test, average='weighted'):.4f}\")\n",
    "\n",
    "# Total Learnable Parameters --------------------\n",
    "\n",
    "def count_parameters(weights, biases):\n",
    "    total = 0\n",
    "    for w, b in zip(weights, biases):\n",
    "        total += np.prod(w.shape) + np.prod(b.shape)\n",
    "    return total\n",
    "\n",
    "print(\"\\nTotal Learnable Parameters:\", count_parameters(weight, bias))\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Peak RAM usage: {peak / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "tracemalloc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
